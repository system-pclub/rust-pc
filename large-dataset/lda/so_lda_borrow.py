# -*- coding: utf-8 -*-
"""SO_lda.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10sIYgTAMaBRk55YLed0kWCtXaUt6rqpu
"""

# python 3.8
import nltk # ver=3.5
import gensim # ver=4.0

# read text, build doc index 2 id map
import pandas as pd
path = "./SO_api_QA_text_de_duplicated_result_processed.json"

current_question_group = 'borrow'
def get_question_id_list():
    ret = set()
    with open(current_question_group+'.txt') as fd:
        for line in fd:
            ret.add(line.strip())
    return ret


def read_data(path):
    data = pd.read_json(path)
    print(data.head())
    id_list = get_question_id_list()
    data = data[data.question_id.isin(id_list)]
    doc_index2id = data.question_id.tolist()
    docs = data.text.tolist()
    return (docs, doc_index2id)


docs, doc_index2id = read_data(path)
doc_text = docs.copy()
#print(doc_text)

#
# docs[:2]
#
# doc_index2id[:3]

# lemmatize
from nltk.corpus import wordnet as wn
from nltk.stem.wordnet import WordNetLemmatizer
from nltk import word_tokenize, pos_tag, sent_tokenize
from collections import defaultdict
from gensim.utils import deaccent
import re, os

def preprocess_docs(docs: list):
    """
    lemmatize and lower, remove punctuations and stopwords
    modify `docs` into a list of docs, each doc is a list of sentences, 
    each sentence is a list of tokens
    """
    from nltk.corpus import stopwords
    stopwords = set(stopwords.words('english'))
    tag_map = defaultdict(lambda : wn.NOUN)
    tag_map['J'] = wn.ADJ
    tag_map['V'] = wn.VERB
    tag_map['R'] = wn.ADV
    
    lemmatizer = WordNetLemmatizer()
    punctuation_pattern = re.compile(r"[!\"#$%&'()*+,/:;<=>?@[\]\\^_`{|}~]|(\.\s*$)|(-+\B)")
    for i, doc in enumerate(docs):
        sentences = sent_tokenize(doc)
        new_sentences = []
        for sentence in sentences:
#             sentence = punctuation_pattern.sub(" ", sentence)
            if type(sentence) != str:
                continue
            sentence = deaccent(sentence)
            tokens =  word_tokenize(sentence)
            if len(tokens) == 0:
                continue
            #lemmatize and lower, then remove punctuations
            tokens = [punctuation_pattern.sub(
                            '',
                            lemmatizer.lemmatize(token, tag_map[tag[0]]).lower() 
                        )
                      for token, tag in pos_tag(tokens)]
            # remove empty or stopwords
            tokens = list(filter(lambda x: (len(x) > 0) and x not in stopwords , tokens))
            if len(tokens) == 0:
                continue
            new_sentences.append(tokens)
        docs[i] = new_sentences
    return docs


#preprocess_docs(docs[:1])

# import pickle
# preprocessed_docs_path = "preprocessed_docs_" + current_question_group + ".pickle"
#
# if os.path.exists(preprocessed_docs_path):
#     with open(path, "rb") as f:
#         docs = pickle.load(f)
# else:
#
#     with open(preprocessed_docs_path, 'wb') as f:
#         pickle.dump(docs, f)

docs = preprocess_docs(docs)
#docs[:2]

def flatten_list(source: list):
    return [item for sublist in source for item in sublist]

flattened_docs = flatten_list(docs)
# flattened_docs[:2]

# don't need this
def get_one_grams(sentences: list, min_count=5):
    from collections import Counter
    """
    sentences: list of list of tokens
    """
    counter = Counter()
    for sentence in sentences:
        counter.update(sentence)
    print(counter.most_common(100))
    return set(key for key, value in counter.items() if value >= min_count)

# arguments
#num_topics=8
random_state=200
chunksize=100
passes=20
iterations=2000
# LdaMultiCore cannot use 'auto'
alpha='auto'
eta='auto'
# alpha='symmetric'
# eta='symmetric'
eval_every=None
per_word_topics=True
min_count = 3
no_above_proportion = 0.5
phrase_threshold = 1.0

# bigram
from gensim.models.phrases import Phrases, Phraser
bigram_model = Phrases(flattened_docs, min_count=min_count, threshold=phrase_threshold)

# threshold = 10.0: num of bigram: 3045
# threshold = 5.0 bigram: 4655
# threshold = 2.0 bigram: 8217

# phrases = bigram_model.export_phrases(flattened_docs)
# two_words = set(key.decode('utf-8') for key, _ in phrases)
# print(len(two_words))

bigram_model = Phraser(bigram_model)

# show
# bigram_model[docs[2][5]]

def bigram_and_process_docs(docs: list, bigram_model):
    """
    return: list of docs, each doc is a list of bigrams and filtered monograms
    """
    new_docs = []
    for doc in docs:
        new_doc = []
        for sentence in doc:
            bigrams = bigram_model[sentence]
            new_doc.extend(bigrams)
        new_docs.append(new_doc)
    return new_docs

filtered_docs = bigram_and_process_docs(docs, bigram_model)

# filtered_docs[:2]

import gensim.corpora as corpora
id2word = corpora.Dictionary(filtered_docs)
id2word.filter_extremes(no_above=no_above_proportion)
corpus = [id2word.doc2bow(text) for text in filtered_docs]
print(corpus[:2])

len(id2word)

range_start = 5
range_end = 30 + 1

# lda
# Enable logging for gensim - optional
from importlib import reload  # Not needed in Python 2
import logging
reload(logging)
logging.basicConfig(filename="lda_train_2.log", format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

def get_topic_docs_map(lda_model, corpus):
    threshold = get_threshold(lda_model, corpus)
    print("threshold: {}".format(threshold))
    topic_docs_map = {}
    for i, doc in enumerate(corpus):
        doc_vector = lda_model[doc]
        #print(doc_vector)
        for topic, prob in doc_vector[0]:
            if prob > threshold:
                if topic in topic_docs_map:
                    topic_docs_map[topic].append((i, prob))
                else:
                    topic_docs_map[topic] = [(i, prob)]
    for value in topic_docs_map.values():
        value.sort(key=lambda x: x[1], reverse=True)
    return topic_docs_map

def get_threshold(lda_model, corpus):
    scores = []
    for doc in corpus:
        doc_vector = lda_model[doc]
        for _topic, prob in doc_vector[0]:
            scores.append(prob)
    return sum(scores) / len(scores)

from pprint import pformat

coherence_map = {}


from gensim.models import LdaMulticore, LdaModel
with open("coherence_data1.txt", "w") as coherence_f:
    coherence_f.write("num of topcis,average topic coherence\n")
    for num_topics in range(range_start, range_end, 1):
        print("start: num of topics:", num_topics)
        lda_model = LdaModel(  
                            corpus=corpus,
                            id2word=id2word,
                            num_topics=num_topics, 
                            random_state=random_state,
                            chunksize=chunksize,
                            passes=passes,
                            iterations=iterations,
                            alpha=alpha,
                            eta=eta,
                            eval_every=eval_every,
                            per_word_topics=per_word_topics)
        top_topics = lda_model.top_topics(corpus) #, num_words=20)
        print(top_topics)
        lda_model.save(f"saved_lda_model_{num_topics}_topics")
        # Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.
        avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics
        print('Average topic coherence: %.4f.' % avg_topic_coherence)
        with open(f"lda_result_{num_topics}_topics.txt", "w") as f:
            f.write(f"average topic coherence: {avg_topic_coherence}\n")
            f.write(pformat(top_topics) + "\n")
        coherence_map[num_topics] = avg_topic_coherence
        coherence_f.write(f"{num_topics},{avg_topic_coherence}\n")
        id2word = lda_model.id2word
        corpus = [id2word.doc2bow(text) for text in filtered_docs]

        doc_lda = lda_model[corpus[0]]
        topic_docs_map = get_topic_docs_map(lda_model, corpus)
        import csv

        with open("top_topics_" + str(num_topics) + "_" + current_question_group + ".csv", "w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow((
                'topic',
                'top words',
                'num of docs',
                'representing docs'
            ))
            topics = lda_model.show_topics(-1, num_words=20, formatted=False)
            for topic_id, words in topics:
                print(words)
                words = [word[0] for word in words]
                words = ", ".join(words)
                related_docs = topic_docs_map.get(topic_id, [])
                related_docs = list(map(lambda x: x[0], related_docs))
                num = len(related_docs)
                docs = [str(doc_index2id[index]) for index in related_docs[:5]]
                writer.writerow(("", words, num, *docs))


# classify which topics a doc belongs to.
# this function is **wrong**. We should classify documents to topics by setting a threshold
# , not find the largest value.
# a doc can belong to multiple topics.
# 张子毅的版本
# def get_topic_docs_map(lda_model, corpus):
#     topic_docs_map = {}
#     for i, doc in enumerate(corpus):
#         doc_vector = lda_model[doc]
#         topic, prob = max(doc_vector[0], key=lambda x: x[1])
#         if topic in topic_docs_map:
#             topic_docs_map[topic].append((i, prob))
#         else:
#             topic_docs_map[topic] = [(i,prob)]
#     for value in topic_docs_map.values():
#         value.sort(key=lambda x: x[1], reverse=True)
#     return topic_docs_map
# 我的版本



